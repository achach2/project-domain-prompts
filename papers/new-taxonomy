The Prompt Report presents a comprehensive new taxonomy of prompting techniques — a structured way to categorize and describe the wide variety of prompting strategies used in language model interactions. This is one of the central contributions of the paper.

⸻

Overview of the Taxonomy

The taxonomy is organized along four dimensions:

⸻

1. Prompt Format

This dimension describes how the prompt is structured. Key categories include:
	•	Zero-Shot Prompting: No examples are given — just the task or question.
	•	Few-Shot Prompting: A few examples are given before the task.
	•	Chain-of-Thought Prompting: Prompts include reasoning steps before the answer.
	•	Instruction Prompting: Prompts contain explicit task instructions.
	•	Exemplar Prompting: Prompts include high-quality demonstrations or templates.

⸻

2. Prompt Content

This focuses on what the prompt includes, such as:
	•	Knowledge Prompts: Embed factual content (e.g. definitions, summaries).
	•	Question Prompts: Pose questions directly to the model.
	•	Multi-Turn Prompts: Include previous user-model interactions (e.g., in chat settings).
	•	Generated Prompts: Use another model or algorithm to construct prompts.

⸻

3. Prompt Source

Describes where the prompt comes from:
	•	Human-Written: Manually created by users or researchers.
	•	Model-Generated: Created by a model (e.g., prompt generation with GPT).
	•	Hybrid: Combination of human and model input.
	•	Retrieved: Sourced from a knowledge base or dataset (e.g., retrieval-augmented prompting).

⸻

4. Prompt Objective

Defines the purpose or use-case of the prompt:
	•	Instruction Following: Designed to guide the model to follow commands.
	•	Task Solving: Focused on solving specific problems (e.g., math, logic).
	•	Reasoning: Prompted to reason step-by-step.
	•	Creative Generation: Aimed at generating narratives, poems, code, etc.
	•	Evaluation: Prompts used to assess or compare model outputs.

⸻

Visual Structure

The paper presents this taxonomy as a layered framework, allowing combinations like:
	•	Few-shot + Chain-of-Thought + Human-Written + Task Solving
	•	Zero-shot + Instruction + Model-Generated + Reasoning

This makes it easier to systematically describe, compare, and reproduce prompting strategies across papers and applications.

⸻
