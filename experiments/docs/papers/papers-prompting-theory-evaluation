1. Prompt Effectiveness: Key Metrics and Tools. Portkey Al Blog, 2024

2. Log Probability as a Detection Method, EACL 2023

3. Prompt Perturbation and Robustness Evaluation, talk, 2023 (slides)

4. Quantifying and Mitigating Prompt Overfitting

5. Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4

6. Galileo Al. A Metrics-First Approach to LLM Evaluation, 2023


To support your goal of deepening prompt engineering and evaluation strategies‚Äîparticularly to understand why and when advanced techniques like ReAct or Tree of Thought (ToT) work‚Äîeach of the following resources contributes unique perspectives or tools. Here‚Äôs how they can inform your research:

‚∏ª

1. Prompt Effectiveness: Key Metrics and Tools

Portkey AI Blog, 2024

How it helps:
	‚Ä¢	Offers a consolidated framework of evaluation metrics for prompting (e.g., coherence, coverage, helpfulness).
	‚Ä¢	Suggests tooling and automation for prompt experimentation (e.g., Portkey‚Äôs playground + dashboards).
	‚Ä¢	Useful for benchmarking ReAct and ToT using consistent metrics beyond traditional NLP scores.

Key Insight:
Introduces practical metrics (like traceability of reasoning or user trust scores) that align with ReAct/ToT‚Äôs goals.

‚∏ª

2. Log Probability as a Detection Method

EACL 2023

How it helps:
	‚Ä¢	Proposes using token-level log probabilities to detect hallucinations or unstable model behavior.
	‚Ä¢	Can be applied to compare ReAct vs. standard prompting in terms of confidence or internal consistency.
	‚Ä¢	Potential for detecting over-assertion in hallucinated ReAct chains.

Key Insight:
Use log-probs to quantify hallucination risk or brittleness in complex reasoning prompts.

‚∏ª

3. Prompt Perturbation and Robustness Evaluation

(Talk, 2023 ‚Äì Slides)

How it helps:
	‚Ä¢	Encourages evaluating how stable prompting techniques are to slight rewording or paraphrasing.
	‚Ä¢	Apply this to test whether ToT or ReAct prompts degrade under perturbation, which would reflect fragility in logic chaining.
	‚Ä¢	Useful for systematically stress-testing templates.

Key Insight:
Test robustness and generalization of structured prompting strategies via perturbation experiments.

‚∏ª

4. Quantifying and Mitigating Prompt Overfitting

How it helps:
	‚Ä¢	Introduces the concept of overfitting to prompt examples, particularly in few-shot and chain-of-thought settings.
	‚Ä¢	Helps distinguish between true reasoning gains and superficial mimicry.
	‚Ä¢	Apply to ensure ReAct responses aren‚Äôt memorized templates but reflect adaptive logic.

Key Insight:
Design prompts that minimize leakage and encourage true task generalization‚Äînot just memorized flows.

‚∏ª

5. Principled Instructions Are All You Need

(For LLaMA, GPT-3.5/4)

How it helps:
	‚Ä¢	Shows how clear, constraint-driven instructions can outperform examples or CoT in some tasks.
	‚Ä¢	A counterpoint to complex prompting‚Äîsuggests that well-structured zero-shot ReAct may suffice in some cases.
	‚Ä¢	Useful to contrast when minimal vs. maximal prompting is ideal.

Key Insight:
Evaluate whether ToT or ReAct is necessary‚Äîor if precise, clean instruction phrasing achieves similar gains.

‚∏ª

6. Galileo AI: A Metrics-First Approach to LLM Evaluation

How it helps:
	‚Ä¢	Focuses on real-time diagnostic feedback, using metrics like toxicity, repetition, hallucination, and specificity.
	‚Ä¢	Provides dashboards and tooling for batch prompt analysis.
	‚Ä¢	Aligns well with your research goal of making ReAct/ToT evaluation more quantitative and automated.

Key Insight:
Adapt Galileo-style scoring and tracking for systematic prompt variant evaluation (e.g., per-step ReAct scoring).

‚∏ª

Conclusion:

Each paper adds a complementary piece:
	‚Ä¢	Portkey + Galileo: For infrastructure and metrics.
	‚Ä¢	Log-probs + Perturbation: For robustness and failure analysis.
	‚Ä¢	Overfitting + Principled Instructions: For theory and constraints.

Together, they give you a robust framework to:
	‚Ä¢	Evaluate why prompting techniques work (or don‚Äôt).
	‚Ä¢	Compare them quantitatively and qualitatively.
	‚Ä¢	Optimize prompt design for robustness and clarity.

--------------
--------------

Here‚Äôs a literature review on prompting theory and evaluation frameworks, synthesizing the key insights from the six papers/resources you‚Äôve listed:

‚∏ª

1. Prompt Effectiveness: Key Metrics and Tools (Portkey AI, 2024)

Contribution: Practical framework for evaluating prompts used in production settings.
Theory: Suggests prompt effectiveness is multidimensional‚Äîdefined by latency, token usage, cost, and output quality.
Evaluation:
	‚Ä¢	Focuses on operational metrics.
	‚Ä¢	Emphasizes observability and real-time tracking via integrated tools.
Limitation: Less theoretical depth; more useful for product teams than academic evaluations.

‚∏ª

2. Log Probability as a Detection Method (EACL 2023)

Contribution: Uses sequence-level log-probability to detect hallucinations in generated content.
Theory: Ties model confidence (via log-probs) to factual correctness.
Evaluation:
	‚Ä¢	Proposes using low log-probability spans to identify hallucinations.
	‚Ä¢	Combines log-probs with human annotations for benchmarking.
Strength: Introduces statistical grounding to prompt evaluation.
Limitation: Focused on hallucination detection, not broad prompt quality.

‚∏ª

3. Prompt Perturbation and Robustness Evaluation (Shuyue Jia, 2023)

Contribution: Investigates prompt robustness through controlled perturbations (e.g., rewording, order shifts).
Theory: Suggests prompts should be semantically stable‚Äîminor changes shouldn‚Äôt yield major output differences.
Evaluation:
	‚Ä¢	Uses output similarity, embedding distance, and human evaluation.
	‚Ä¢	Identifies model fragility across prompt variants.
Strength: Bridges prompt formulation and robustness testing.
Limitation: Doesn‚Äôt provide a formal scoring framework.

‚∏ª

4. Quantifying and Mitigating Prompt Overfitting (Aissi et al., 2024)

Contribution: Introduces a Prompt Sensitivity Score to quantify how much LLMs overfit to prompt wording post-RLHF.
Theory: Suggests that prompts can become a ‚Äúcrutch,‚Äù and that LLMs should generalize across paraphrased prompts.
Evaluation:
	‚Ä¢	Evaluates models on prompt variance and response stability.
	‚Ä¢	Includes ablation studies to test mitigation strategies.
Strength: Introduces formal sensitivity metrics for evaluation.
Limitation: Requires specialized setup with interactive environments.

‚∏ª

5. Principled Instructions Are All You Need (2024)

Contribution: Demonstrates that carefully written principled instructions outperform complex prompting tricks across LLaMA-1/2, GPT-3.5/4.
Theory: The clarity and structure of instructions are more important than task-specific tuning.
Evaluation:
	‚Ä¢	Compares prompt performance using standard NLP benchmarks (e.g., ARC, HellaSwag).
	‚Ä¢	Studies cross-model generalization of instruction quality.
Strength: Argues for prompt minimalism and clarity.
Limitation: May underplay benefits of task-specific prompting.

‚∏ª

6. A Metrics-First Approach to LLM Evaluation (Galileo AI, 2023)

Contribution: Proposes observability-first evaluation, with dashboards for LLM performance tracking.
Theory: Shifts the framing of prompt evaluation to continuous feedback loops.
Evaluation:
	‚Ä¢	Uses human-in-the-loop scoring, categorization of errors, and output tracking over time.
	‚Ä¢	Focus on enterprise workflows.
Strength: Emphasizes real-world feedback.
Limitation: Not suited for reproducible research benchmarks.

‚∏ª

Synthesis: Emerging Theoretical Frameworks

Dimension	Key Idea	Leading Papers
Clarity	Structured, principled instructions outperform hacks	Principled Instructions
Robustness	Prompts should be stable to minor rewording	Prompt Perturbation
Generalization	Avoid overfitting to specific phrasing	Prompt Overfitting
Quantifiability	Move beyond subjective judgments	Log Probabilities, Prompt Sensitivity Score
Practicality	Real-time tools and dashboards matter	Portkey, Galileo



‚∏ª


log probability as detection method 

the paper titled ‚ÄúLog Probability as a Detection Method‚Äù was presented at the 17th Conference of the European Chapter of the Association for Computational Linguistics (EACL) in 2023. This study investigates the efficacy of using log probability scores to detect anomalies or inconsistencies in language model outputs.

Key Highlights:
	‚Ä¢	Detection Mechanism: The authors propose leveraging the log probability scores generated by language models to identify outputs that deviate from expected patterns. This method aims to detect instances where the model‚Äôs confidence in its output is unusually low, which may indicate errors or anomalies.
	‚Ä¢	Evaluation: The paper evaluates this detection method across various tasks and datasets, demonstrating its potential in identifying problematic outputs without the need for extensive labeled data.

This approach offers a lightweight and model-agnostic technique for enhancing the reliability of language model outputs, particularly useful in scenarios where traditional evaluation metrics are insufficient.

Here‚Äôs an intuitive explanation of how log probability can be used as a detection mechanism, followed by a concrete example.

‚∏ª

üîç What is Log Probability in Language Models?

When an LLM like GPT generates text, it predicts each next token (word/character) with a probability. Log probability is just the logarithm of this token-level probability, and it‚Äôs often summed or averaged over the entire sequence to measure how confident the model was in producing that response.
	‚Ä¢	High log probability (less negative) = model was confident.
	‚Ä¢	Low log probability (more negative) = model was unsure or generating rare/unexpected text.

‚∏ª

‚úÖ Why Use It for Detection?

If the model generates a response with unusually low average log probability, it may signal:
	‚Ä¢	A hallucination
	‚Ä¢	A semantic drift
	‚Ä¢	An off-topic or incoherent response

So you can flag or re-rank low-confidence responses automatically‚Äîeven without labeled training data.

‚∏ª

üß† Example: Prompt + Two Candidate Responses

Prompt:

‚ÄúSummarize the main function of mitochondria.‚Äù

Candidate Responses:

A.

‚ÄúMitochondria generate ATP, the energy currency of the cell, through cellular respiration.‚Äù

B.

‚ÄúMitochondria are brain tissues found in the respiratory system that help with digestion.‚Äù

‚∏ª

LLM Log Probabilities:
	‚Ä¢	A. Log probability per token: [-0.5, -0.6, -0.4, -0.3, -0.5, -0.4] ‚Üí Average: -0.45
	‚Ä¢	B. Log probability per token: [-1.2, -1.5, -1.0, -1.7, -1.8, -2.0] ‚Üí Average: -1.53

‚∏ª

üìä Interpretation:

Response	Content Quality	Avg Log Prob	Interpretation
A	Correct & fluent	-0.45	High-confidence, likely valid
B	Incoherent	-1.53	Low-confidence, likely hallucinated



‚∏ª

üß™ Applications of This Mechanism:
	1.	Auto-filtering: Discard low-confidence completions.
	2.	Self-refinement: Retry generation if confidence is low.
	3.	Comparative ranking: Choose the most probable among multiple responses.
	4.	Anomaly detection: Detect off-topic or made-up facts in open-ended generation.

‚∏ª


It looks like the transformers library isn‚Äôt installed in this environment. However, you can easily run this on your local machine or Colab. Here‚Äôs the complete Python code to copy and run:

‚∏ª

‚úÖ Python Code: Using Log Probabilities to Evaluate Responses

# Install the required package first
# !pip install transformers torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load model and tokenizer
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.eval()

# Prompt and two responses
prompt = "Summarize the main function of mitochondria."
responses = {
    "A": " Mitochondria generate ATP, the energy currency of the cell, through cellular respiration.",
    "B": " Mitochondria are brain tissues found in the respiratory system that help with digestion."
}

def compute_avg_log_prob(prompt, response):
    input_text = prompt + response
    inputs = tokenizer(input_text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    total_log_prob = -outputs.loss.item() * inputs["input_ids"].size(1)
    avg_log_prob = total_log_prob / len(response.split())
    return avg_log_prob

# Calculate for both responses
for label, response in responses.items():
    avg_log_prob = compute_avg_log_prob(prompt, response)
    print(f"Response {label} - Avg Log Probability: {avg_log_prob:.4f}")



‚∏ª

üîç Expected Output:
	‚Ä¢	Response A should have a higher log probability (more confident).
	‚Ä¢	Response B should show a lower log probability (model uncertainty due to factual errors).


GPT-4o is accessed via the OpenAI API, not via Hugging Face Transformers, and it doesn‚Äôt expose token-level log probabilities by default. However, you can request logprobs during generation using the logprobs parameter in the OpenAI API.

Here‚Äôs how to rewrite the code for GPT-4o using OpenAI API to compute approximate log probabilities for two responses:

‚∏ª

‚úÖ Python Code (OpenAI API + logprobs)

# First, install OpenAI SDK if needed
# pip install openai

import openai
import os

# Set your OpenAI API key
openai.api_key = os.getenv("OPENAI_API_KEY")  # Or hardcode: openai.api_key = "your-key"

# Prompt and responses
prompt = "Summarize the main function of mitochondria."
responses = {
    "A": "Mitochondria generate ATP, the energy currency of the cell, through cellular respiration.",
    "B": "Mitochondria are brain tissues found in the respiratory system that help with digestion."
}

def compute_log_probs(response_text):
    try:
        result = openai.Completion.create(
            model="gpt-4o",
            prompt=prompt,
            max_tokens=len(response_text.split()),
            temperature=0.0,
            logprobs=1,  # request token logprobs
            echo=True
        )

        # Extract logprobs for the response portion (not the prompt part)
        tokens = result['choices'][0]['logprobs']['tokens']
        token_logprobs = result['choices'][0]['logprobs']['token_logprobs']

        # Find where the response begins
        response_start_index = next(
            i for i, token in enumerate(tokens)
            if response_text.strip().startswith(token.strip())
        )

        response_logprobs = token_logprobs[response_start_index:response_start_index + len(response_text.split())]
        avg_logprob = sum(lp for lp in response_logprobs if lp is not None) / len(response_logprobs)

        return avg_logprob

    except Exception as e:
        print(f"Error: {e}")
        return None

# Compute for both responses
for label, text in responses.items():
    avg_log_prob = compute_log_probs(text)
    print(f"Response {label} - Avg Log Probability: {avg_log_prob:.4f}")



‚∏ª

Notes:
	‚Ä¢	GPT-4o may truncate or tokenize differently, so align tokens carefully.
	‚Ä¢	This approach uses echo=True to get token-level logprobs for the full prompt + completion.
	‚Ä¢	For precision, use max_tokens to match the number of tokens in the target response.

‚∏ª

 To compute log probabilities for candidate responses using the LLaMA 3.2 3B model, you‚Äôll need to:
	‚Ä¢	Use Hugging Face‚Äôs transformers library.
	‚Ä¢	Ensure you have access to a causal language model version of LLaMA 3.2 3B.
	‚Ä¢	Compute log probabilities token-by-token using the model‚Äôs loss output.

Here‚Äôs how you can adapt the earlier example for LLaMA 3.2 3B:

‚∏ª

‚úÖ Python Code: Log Probability for LLaMA 3.2 3B

# Install dependencies if needed:
# pip install transformers torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load LLaMA 3.2 3B (replace with actual Hugging Face model ID)
model_name = "meta-llama/Meta-Llama-3-3B"  # Or your preferred checkpoint
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
model.eval()

# Prompt and two candidate responses
prompt = "Summarize the main function of mitochondria."
responses = {
    "A": " Mitochondria generate ATP, the energy currency of the cell, through cellular respiration.",
    "B": " Mitochondria are brain tissues found in the respiratory system that help with digestion."
}

def compute_avg_log_prob(prompt, response):
    # Combine prompt and response
    full_input = prompt + response
    inputs = tokenizer(full_input, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    
    # Compute total log-likelihood and normalize
    log_likelihood = -outputs.loss.item() * inputs["input_ids"].size(1)
    avg_log_prob = log_likelihood / len(response.split())
    return avg_log_prob

# Evaluate responses
for label, response in responses.items():
    avg_logprob = compute_avg_log_prob(prompt, response)
    print(f"Response {label} - Avg Log Probability: {avg_logprob:.4f}")



‚∏ª

üîç How It Works:
	‚Ä¢	Uses the model‚Äôs loss, which is a negative log-likelihood.
	‚Ä¢	Multiplies loss by number of tokens to get total log probability.
	‚Ä¢	Averages over the response length to get average log prob per token.

‚∏ª

Here‚Äôs how to adapt the code to compute average log probability for candidate responses using the Hugging Face model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B:

‚∏ª

‚úÖ Python Code for DeepSeek-R1-Distill-Qwen-7B

# Install dependencies if needed:
# pip install transformers torch

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the DeepSeek model and tokenizer
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)
model.eval()

# Example prompt and responses
prompt = "Summarize the main function of mitochondria."
responses = {
    "A": " Mitochondria generate ATP, the energy currency of the cell, through cellular respiration.",
    "B": " Mitochondria are brain tissues found in the respiratory system that help with digestion."
}

def compute_avg_log_prob(prompt, response):
    full_text = prompt + response
    inputs = tokenizer(full_text, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    
    # Total negative log likelihood
    log_likelihood = -outputs.loss.item() * inputs["input_ids"].size(1)
    
    # Approximate per-token log probability over response
    avg_log_prob = log_likelihood / len(response.split())
    return avg_log_prob

# Run for both responses
for label, response in responses.items():
    avg_log_prob = compute_avg_log_prob(prompt, response)
    print(f"Response {label} - Avg Log Probability: {avg_log_prob:.4f}")



‚∏ª

Notes:
	‚Ä¢	We use trust_remote_code=True because the Qwen model family often uses custom architectures.
	‚Ä¢	This approach gives an approximate confidence score per response, helpful for flagging hallucinations or low-confidence outputs.

‚∏ª

The environment currently doesn‚Äôt support torch execution due to missing GPU-related shared libraries. However, here‚Äôs a clean and ready-to-run version of the code that you can use locally or in a Colab notebook:

‚∏ª

‚úÖ Local/Colab Wrapper for Scoring and Ranking LLM Outputs

# Install if needed:
# pip install transformers torch pandas

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import pandas as pd

# Define models
models_info = {
    "LLaMA 3.2 3B": "meta-llama/Meta-Llama-3-3B",
    "DeepSeek Qwen 7B": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
}

# Load models
loaded_models = {}
for name, model_id in models_info.items():
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
    model.eval()
    loaded_models[name] = {"tokenizer": tokenizer, "model": model}

# Define prompt and responses
prompt = "Summarize the main function of mitochondria."
responses = {
    "A": " Mitochondria generate ATP, the energy currency of the cell, through cellular respiration.",
    "B": " Mitochondria are brain tissues found in the respiratory system that help with digestion."
}

# Score responses using log probability
def compute_avg_log_prob(prompt, response, tokenizer, model):
    full_text = prompt + response
    inputs = tokenizer(full_text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    total_logprob = -outputs.loss.item() * inputs["input_ids"].size(1)
    return total_logprob / len(response.split())

# Collect scores
results = []
for model_name, model_data in loaded_models.items():
    for label, response in responses.items():
        avg_lp = compute_avg_log_prob(prompt, response, model_data["tokenizer"], model_data["model"])
        results.append({"model": model_name, "response": label, "avg_log_prob": avg_lp})

# Add GPT-4o (simulated)
results += [
    {"model": "GPT-4o", "response": "A", "avg_log_prob": -0.45},
    {"model": "GPT-4o", "response": "B", "avg_log_prob": -1.53}
]

# Create and sort dataframe
df = pd.DataFrame(results)
df_sorted = df.sort_values(by=["response", "avg_log_prob"], ascending=[True, False])
print(df_sorted)



‚∏ª

üß† What It Does:
	‚Ä¢	Scores each response with average log probability.
	‚Ä¢	Sorts outputs within each response (A, B) from most to least confident.
	‚Ä¢	Helps flag hallucinated or low-confidence answers automatically.


Here is the full Python code that evaluates and plots average log probability comparisons across three models (GPT-4o, LLaMA 3.2 3B, and DeepSeek Qwen 7B) for two responses to a given prompt:

‚∏ª

‚úÖ Python Code: Compare and Plot Log Probabilities

import matplotlib.pyplot as plt
import pandas as pd

# Simulated results (replace with real scores if running full scoring wrapper)
results = [
    {"model": "GPT-4o", "response": "A", "avg_log_prob": -0.45},
    {"model": "GPT-4o", "response": "B", "avg_log_prob": -1.53},
    {"model": "LLaMA 3.2 3B", "response": "A", "avg_log_prob": -0.60},
    {"model": "LLaMA 3.2 3B", "response": "B", "avg_log_prob": -1.40},
    {"model": "DeepSeek Qwen 7B", "response": "A", "avg_log_prob": -0.55},
    {"model": "DeepSeek Qwen 7B", "response": "B", "avg_log_prob": -1.35}
]

# Create DataFrame
df = pd.DataFrame(results)

# Pivot for grouped bar plot
pivot_df = df.pivot(index="model", columns="response", values="avg_log_prob")

# Plotting
ax = pivot_df.plot(kind="bar", figsize=(10, 6), color={"A": "green", "B": "red"})
plt.title("Log Probability Comparison for Two Responses Across LLMs")
plt.ylabel("Average Log Probability (Higher = More Confident)")
plt.xticks(rotation=0)
plt.axhline(0, color="gray", linewidth=0.8)
plt.legend(title="Response")
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()



‚∏ª

What You‚Äôll See:
	‚Ä¢	Bars grouped by model (GPT-4o, LLaMA 3.2 3B, DeepSeek Qwen 7B)
	‚Ä¢	Green = confident (Response A)
	‚Ä¢	Red = hallucinated/low-confidence (Response B)
	‚Ä¢	Clear comparison of how each model scored the same prompt+response pair

===========================
===========================
===========================

Galileo Al. A Metrics-First Approach to LLM Evaluation, 2023

The article ‚ÄúA Metrics-First Approach to LLM Evaluation‚Äù by Galileo AI outlines a comprehensive framework for assessing Large Language Models (LLMs) using a variety of metrics. This approach emphasizes the importance of quantitative evaluation to ensure the reliability and effectiveness of LLMs in various applications.

Key Evaluation Metrics:
	1.	Context Adherence: Measures how well the model‚Äôs response aligns with the provided context, ensuring factual accuracy and minimizing hallucinations.
	2.	Correctness: Assesses the factual accuracy of the model‚Äôs output, identifying errors of precision. Ôøº
	3.	LLM Uncertainty: Utilizes log probability to detect low-quality generations, with higher uncertainty indicating potential inaccuracies. Ôøº
	4.	Prompt Perplexity: Evaluates the model‚Äôs understanding of the input prompt, where lower perplexity suggests better comprehension.
	5.	Context Relevance: In Retrieval-Augmented Generation (RAG) systems, this metric assesses the relevance of retrieved documents to the user‚Äôs query.
	6.	Completeness: Determines whether the model‚Äôs response fully addresses all relevant information from the context. Ôøº
	7.	Chunk Attribution: Analyzes which parts of the retrieved context contributed to the model‚Äôs response, aiding in understanding and improving RAG systems.
	8.	Chunk Utilization: Measures how effectively the model uses the retrieved context in generating its response. Ôøº
	9.	Safety Metrics: Evaluates the presence of sensitive information, toxic content, and other safety concerns in the model‚Äôs output. Ôøº
	10.	Custom Evaluation Metrics: Allows for the development of tailored metrics to address specific use cases and requirements.

Galileo‚Äôs approach underscores the necessity of integrating these metrics into the development and deployment of LLMs to enhance their reliability and trustworthiness. By systematically evaluating models across these dimensions, organizations can better understand model behavior, identify areas for improvement, and ensure that LLM applications meet desired standards of performance and safety.

For a more detailed exploration of these metrics and their applications, you can refer to the full article here: A Metrics-First Approach to LLM Evaluation.

Galileo AI‚Äôs metrics like context adherence, correctness, LLM uncertainty (log probability), and completeness are not yet available as a public Python package. However, we can simulate a basic implementation of these metrics using the OpenAI API and common evaluation logic:

‚∏ª

‚úÖ Python Code: Simulate Galileo-Style Metrics for GPT-4o

# Install OpenAI SDK if needed
# pip install openai

import openai
import os

openai.api_key = os.getenv("OPENAI_API_KEY")  # or replace with your key

# Inputs
instruction = "Summarize the main function of mitochondria."
context = "Mitochondria are known as the powerhouse of the cell. They generate ATP through cellular respiration."
response = "Mitochondria generate ATP, the energy currency of the cell, through cellular respiration."

# 1. Context Adherence (Simulated via keyword overlap)
def context_adherence(response, context):
    return len(set(response.lower().split()) & set(context.lower().split())) / len(set(response.lower().split()))

# 2. LLM Uncertainty (Log prob via OpenAI API)
def log_prob_gpt4o(prompt, response):
    full_prompt = prompt + "\n" + response
    result = openai.Completion.create(
        model="gpt-4o",
        prompt=full_prompt,
        max_tokens=0,
        temperature=0,
        logprobs=1,
        echo=True
    )
    token_logprobs = result['choices'][0]['logprobs']['token_logprobs']
    response_logprobs = token_logprobs[-len(response.split()):]
    avg_log_prob = sum(lp for lp in response_logprobs if lp is not None) / len(response_logprobs)
    return avg_log_prob

# 3. Completeness (Simulated: Did the response touch all key concepts?)
expected_keywords = ["ATP", "energy", "cell", "respiration"]
def completeness(response):
    matched = [kw for kw in expected_keywords if kw.lower() in response.lower()]
    return len(matched) / len(expected_keywords)

# Run evaluations
metrics = {
    "context_adherence": context_adherence(response, context),
    "completeness": completeness(response),
    # Uncomment below after inserting OpenAI key
    # "log_prob_gpt4o": log_prob_gpt4o(instruction + "\n" + context, response)
}

# Display results
for k, v in metrics.items():
    print(f"{k}: {v:.2f}")



‚∏ª

üìä What It Evaluates:
	‚Ä¢	Context Adherence: How many words in the response match the source context.
	‚Ä¢	Completeness: Did it hit all important expected ideas.
	‚Ä¢	LLM Uncertainty: Uses log probability from GPT-4o to see how confident the model was (lower = more confident).

‚∏ª




