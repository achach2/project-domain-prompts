import json
import re

def evaluate_context_quality(context, validation_instruction, max_retries):
    """
    Evaluate retrieved context against a validation instruction using:
    - Relevancy (50%)
    - Completeness (30%)
    - Specificity (20%)
    Each criterion includes both a numeric score (1–5) and justification.
    """

    evaluation_prompt = f"""
Evaluate the following retrieved context in relation to the validation instruction using three criteria:

1. Relevancy – How directly does the context address the instruction?
2. Completeness – Are all required elements present to answer the instruction fully?
3. Specificity – Does the context cite specific items (terms, metrics, definitions) from the document?

Validation Instruction:
"{validation_instruction}"

Retrieved Context:
"{context}"

Return your evaluation as a JSON object in the following format:

{{
  "Relevancy": {{
    "Score": X,
    "Justification": "Explanation..."
  }},
  "Completeness": {{
    "Score": Y,
    "Justification": "Explanation..."
  }},
  "Specificity": {{
    "Score": Z,
    "Justification": "Explanation..."
  }}
}}

Where X, Y, Z are scores from 1 (very poor) to 5 (excellent).
Do not include any commentary outside the JSON structure.
"""

    for attempt in range(max_retries):
        response = call_llm(
            evaluation_prompt,
            temperature=0.3,
            max_tokens=MAX_TOKENS,
            system_prompt=SYSTEM_PROMPT
        )

        try:
            # Remove markdown code block markers
            clean_response = re.sub(r"```(?:json)?", "", response, flags=re.IGNORECASE).strip()

            print(f"[DEBUG] Cleaned response before parsing:\n{clean_response}\n")

            scores = json.loads(clean_response)

            # Validate keys
            for metric in ["Relevancy", "Completeness", "Specificity"]:
                if metric not in scores or "Score" not in scores[metric]:
                    raise ValueError(f"Missing score for {metric}")

            avg_score = (
                scores["Relevancy"]["Score"] * 0.5 +
                scores["Completeness"]["Score"] * 0.3 +
                scores["Specificity"]["Score"] * 0.2
            )

            explanations = {
                "Relevancy": scores["Relevancy"]["Justification"],
                "Completeness": scores["Completeness"]["Justification"],
                "Specificity": scores["Specificity"]["Justification"]
            }

            return avg_score, scores, explanations

        except Exception as e:
            print(f"[Error] Attempt {attempt + 1} failed during evaluation: {e}")
            print(f"[Raw response]:\n{response}\n")

    # Fallback
    print("[Fallback] All evaluation attempts failed. Returning default low scores.")
    return 1, {
        "Relevancy": {"Score": 1, "Justification": "Parsing failed."},
        "Completeness": {"Score": 1, "Justification": "Parsing failed."},
        "Specificity": {"Score": 1, "Justification": "Parsing failed."}
    }, {
        "Relevancy": "Parsing failed.",
        "Completeness": "Parsing failed.",
        "Specificity": "Parsing failed."
    }

def react_validation_assessment(validation_instruction, text):
    reasoning_steps = []
    context = ""
    iteration = 0
    relevancy_score = 0

    while relevancy_score < RELEVANCY_THRESHOLD and iteration < MAX_ITERATIONS:
        iteration += 1
        print("\n" + "=" * 60)
        print(f"Iteration {iteration}...")

        # 1. Thought Generation
        thought_prompt = f"""
Given the validation instruction: "{validation_instruction}"
and the retrieved context: "{context}"
What additional information is needed for a thorough assessment? Provide a concise answer in one sentence.
"""
        system_prompt = "Provide a concise answer in 1–3 sentences maximum."
        thought = call_llm(thought_prompt, temperature=0.3, max_tokens=500, system_prompt=system_prompt)
        print("Generated Thought:\n", thought)
        reasoning_steps.append(f"Thought (iteration {iteration}): {thought}")

        # 2. Action Selection (Query Formulation)
        action_prompt_for_query_formulation = f"""
Based on the thought: "{thought}",
formulate a query to retrieve missing contextual details from the Model Development Document.
"""
        action_1 = call_llm(action_prompt_for_query_formulation, temperature=0.3, max_tokens=500, system_prompt=system_prompt)
        print("Generated Action (Query):\n", action_1)
        reasoning_steps.append(f"Action (iteration {iteration}) (Query Formulation): {action_1}")

        # 3. Retrieve Context (simulated)
        action_prompt_retrieve_context = f"""
Answer the query using the provided CONTEXT.
QUERY: {action_1}
CONTEXT: "{text}"
"""
        additional_context = call_llm(action_prompt_retrieve_context, temperature=0.3, max_tokens=500, system_prompt=system_prompt)
        print("Retrieved Additional Context:\n", additional_context)
        reasoning_steps.append(f"Action (iteration {iteration}) (Context Retrieved): {additional_context}")

        # 4. Evaluate Retrieved Context Quality
        avg_score, scores, explanations = evaluate_context_quality(additional_context, validation_instruction, max_retries=3)
        relevancy_score = avg_score

        print("\nContext Evaluation Scores:")
        for metric, score_obj in scores.items():
            explanation = explanations.get(metric, "")
            print(f"{metric}: {score_obj['Score']}  {explanation}")
        print(f"Average Relevancy Score: {avg_score}")

        if relevancy_score >= RELEVANCY_THRESHOLD:
            print("\nSufficient relevant context retrieved. Proceeding to report generation...")
            context += "\n" + additional_context
            break
        else:
            print("\nContext not relevant enough, refining search...")
            time.sleep(2)

    # 5. Final Report Generation
    report_prompt = f"""
Based on the final observations and retrieved context, generate a structured validation report.

Validation Assessment: {validation_instruction}
Context: {context}

Provide a detailed and structured response.
"""
    validation_report = call_llm(report_prompt, temperature=0.7, max_tokens=2000, system_prompt=system_prompt)
    print("Generated Validation Report:\n", validation_report)

    return validation_report, reasoning_steps
